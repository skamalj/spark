{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Install Java (I am on 11), Spark and Python (I have 3.8). \n",
    "\n",
    "Install Jupyter server and jupyter plugin in vscode.\n",
    "\n",
    "Set env variables as below (Change values according to your setup)\n",
    "\n",
    "```\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "export SPARK_HOME=/home/kamal/spark-3.1.2-bin-hadoop3.2\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "export PYTHONPATH=${SPARK_HOME}/python/:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):${PYTHONPATH}\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Sessions\n",
    "## Create Spark Session"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"airline\").getOrCreate())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 18:11:52 WARN Utils: Your hostname, kamal-Lenovo-G505 resolves to a loopback address: 127.0.1.1; using 192.168.1.14 instead (on interface wlp3s0)\n",
      "21/09/26 18:11:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kamal/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/26 18:11:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Delta spark session\n",
    "* This requires delta lake jar be downloaded from mavenrepository and placed in SPARK_HOME/jars. Restart Jupyter server after that.  \n",
    "* Install delta-spark package (pip install delta-spark)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "delta_spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and operate on Data\n",
    "## Departure Delays - Read CSV with string schema\n",
    "Data used here is available from databricks github [page](https://github.com/databricks/LearningSparkV2.git)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "csv_file = \"./databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "schema = \"date string, delay int, distance int, origin string, destination string\"\n",
    "df = (spark.read\n",
    ".schema(schema)\n",
    ".option(\"header\", \"true\")\n",
    ".csv(csv_file)).cache()\n",
    "df.printSchema()\n",
    "df.first()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(date='01011245', delay=6, distance=602, origin='ABE', destination='ATL')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Operate on Columns, with col"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df.select(col(\"distance\") / 1000).show(2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+\n",
      "|(distance / 1000)|\n",
      "+-----------------+\n",
      "|            0.602|\n",
      "|            0.369|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using withColumn() to add new computed column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "df.withColumn(\"calculatedCol\", col(\"distance\")/1000).show(2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+--------+------+-----------+-------------+\n",
      "|    date|delay|distance|origin|destination|calculatedCol|\n",
      "+--------+-----+--------+------+-----------+-------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|        0.602|\n",
      "|01020600|   -8|     369|   ABE|        DTW|        0.369|\n",
      "+--------+-----+--------+------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timestamp from millisecond (since epoch)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "spark.sql(\"select timestamp_millis(1631163333673) as timestampdata\").show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+\n",
      "|timestampdata          |\n",
      "+-----------------------+\n",
      "|2021-09-09 10:25:33.673|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with JSON\n",
    "\n",
    "### Extract with from_json()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "data = [(1, '''{\"a\": 1,\"b\": 2.3}''')]\n",
    "schema = 'a INT, b FLOAT'\n",
    "df_json = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df_json.select(from_json(\"value\", schema)[\"a\"].alias(\"id\"), \n",
    "from_json(df_json.value, schema)[\"b\"].alias(\"value\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|  2.3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract with get_json_object()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df_json.select(get_json_object(df_json.value,\"$.a\").alias(\"id\"), \n",
    "get_json_object(\"value\",\"$.b\").alias(\"value\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|  2.3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df.printSchema()\n",
    "from pyspark.sql.functions import col\n",
    "df.select(\"distance\",\"origin\",\"destination\").filter(col(\"distance\") > 1000).distinct().show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    1036|   DFW|        DCA|\n",
      "|    2115|   EWR|        PDX|\n",
      "|    1853|   EWR|        PHX|\n",
      "|    2071|   KOA|        SJC|\n",
      "|    1014|   MDW|        FLL|\n",
      "+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Views\n",
    "* Global views are used, in queries, with \"global_temp\" prefix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "df.createOrReplaceTempView(\"Airline\")\n",
    "df.createOrReplaceGlobalTempView(\"GlobalAirlineView\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "spark.sql(\"select * from Airline limit 2\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination, \n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay >= 120 AND delay <= 360 THEN 'Long Delays'\n",
    "                  WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM Airline\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "spark.sql(\"\"\"SELECT *,\n",
    "                CASE\n",
    "                    WHEN delay < 0 THEN 'EARLY'\n",
    "                    WHEN delay == 0 THEN 'ON-TIME'\n",
    "                    ELSE 'DELAY'\n",
    "                END AS FL_Delay\n",
    "             FROM Airline\"\"\").show(6)   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+--------+------+-----------+--------+\n",
      "|    date|delay|distance|origin|destination|FL_Delay|\n",
      "+--------+-----+--------+------+-----------+--------+\n",
      "|01011245|    6|     602|   ABE|        ATL|   DELAY|\n",
      "|01020600|   -8|     369|   ABE|        DTW|   EARLY|\n",
      "|01021245|   -2|     602|   ABE|        ATL|   EARLY|\n",
      "|01020605|   -4|     602|   ABE|        ATL|   EARLY|\n",
      "|01031245|   -4|     602|   ABE|        ATL|   EARLY|\n",
      "|01030605|    0|     602|   ABE|        ATL| ON-TIME|\n",
      "+--------+-----+--------+------+-----------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add column to DF\n",
    "## Use multiple conditions on column with when-otherwise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn('FL_delay', when(col(\"delay\") > 0, 'DELAYED')\n",
    ".when(col(\"delay\") < 0, 'EARLY')\n",
    ".otherwise('ON-TIME')).show(6)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----+--------+------+-----------+--------+\n",
      "|    date|delay|distance|origin|destination|FL_delay|\n",
      "+--------+-----+--------+------+-----------+--------+\n",
      "|01011245|    6|     602|   ABE|        ATL| DELAYED|\n",
      "|01020600|   -8|     369|   ABE|        DTW|   EARLY|\n",
      "|01021245|   -2|     602|   ABE|        ATL|   EARLY|\n",
      "|01020605|   -4|     602|   ABE|        ATL|   EARLY|\n",
      "|01031245|   -4|     602|   ABE|        ATL|   EARLY|\n",
      "|01030605|    0|     602|   ABE|        ATL| ON-TIME|\n",
      "+--------+-----+--------+------+-----------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set shuffle partitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",4)\n",
    "df.write.option(\"path\",\"/home/kamal/airline_data\").format(\"csv\").saveAsTable(\"Airline\")\n",
    "df.rdd.getNumPartitions()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create, Use and drop databases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.sql(\"drop database if exists newdb cascade\")\n",
    "spark.sql(\"create database newdb location '/home/kamal/spark-warehouse/newdatabase'\")\n",
    "spark.catalog.listDatabases()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/kamal/Documents/dev/spark/spark_notebook/spark-warehouse'),\n",
       " Database(name='newdb', description='', locationUri='file:/home/kamal/spark-warehouse/newdatabase')]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "spark.sql(\"use newdb\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "df.write.saveAsTable(\"airline_date\", mode=\"overwrite\", format=\"parquet\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "spark.sql(\"select * from airline_date where origin = 'SFO'\").createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global\")\n",
    "spark.sql(\"select * from global_temp.us_origin_airport_SFO_global\").count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39483"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "spark.sql(\"show views\").show()\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "|         | airline|       true|\n",
      "+---------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working with AVRO\n",
    "Avro package is still external (but supported), so needs to be downloaded and placed in classpath. Download spark-avro jar from [mavenrepo](https://mvnrepository.com/artifact/org.apache.spark/spark-avro_2.12/3.1.2),  and place it in $SPARK_HOME/jars. Then restart Jupyter server "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.conf.set(\"spark.jars.packages\", \"com.databricks:spark-avro_2.12:3.1.2\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n",
    "    USING com.databricks.spark.avro\n",
    "    OPTIONS (\n",
    "      path \"./databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "    )\n",
    "\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_fl_summary = spark.sql(\"select * from episode_tbl\")\n",
    "df_fl_summary.count()\n",
    "df_fl_summary.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UDFs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import LongType\n",
    "import pandas as pd\n",
    "def cubed(s: pd.Series) -> pd.Series:\n",
    "    return s*s*s\n",
    "\n",
    "df_range = spark.range(1,9)    \n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_range.select(\"id\",cubed_udf(\"id\").alias(\"cubed\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.udf.register(\"cubed_udf\",cubed_udf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_range.createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql(\"select id, cubed_udf(id) from udf_test\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delta Read and Write"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = delta_spark.sql(\"select * from global_temp.GlobalAirlineView\").cache()\n",
    "data.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"/tmp/delta-table/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spark Streaming\n",
    "\n",
    "## foreachBatch Function\n",
    "\n",
    "foreachBatch lets us use \"update\", \"append\" and \"complete\" output modes. Whereas file sink only allows append."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import partial\n",
    "def foreachBatchFunction(format, location, df, batchId):\n",
    "        (df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(format)\n",
    "        .save(location+str(batchId)))\n",
    "writeMicroBatchUpdateToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonUpdate/\")\n",
    "writeMicroBatchAppendToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonAppend/\")\n",
    "writeMicroBatchCompleteToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonComplete/\")\n",
    "writeMicroBatchAppendToJSON10Sec = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonAppend10sec/\")\n",
    "writeMicroBatchToCSV = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/sparkcsv/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Count\n",
    "Generate test dat using [datastreamer](https://github.com/skamalj/datagenerator) - with followung config. It creates space separated list of  6 words in each line\n",
    ">`lorem|words|textline|6`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sparkstream = (SparkSession.builder\n",
    ".appName(\"airline\").getOrCreate())\n",
    "lines = (sparkstream\n",
    "  .readStream.format(\"socket\")\n",
    "  .option(\"host\", \"localhost\")\n",
    "  .option(\"port\", 4000)\n",
    "  .load())\n",
    "\n",
    "linesnojson = lines.select(get_json_object(\"value\",\"$.textline\").alias(\"line\"))\n",
    "\n",
    "words = linesnojson.select(explode(split(col(\"line\"), \" \")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "\n",
    "checkpointDir = \"/home/kamal/sparkcheckpoint\"\n",
    "checkpointFileSyncDir = \"/home/kamal/sparkcheckpointcsv\"\n",
    "\n",
    "#streamingQuery = (counts\n",
    "#  .writeStream\n",
    "#  .format(\"console\")\n",
    "#  .outputMode(\"complete\")\n",
    "#  .trigger(processingTime=\"20 second\")\n",
    "#  .option(\"checkpointLocation\", checkpointDir)\n",
    "#  .start())\n",
    "\n",
    "streamingQueryFile = (counts\n",
    "  .writeStream\n",
    "  .foreachBatch(writeMicroBatchToCSV)\n",
    "  .outputMode(\"update\")\n",
    "  .trigger(processingTime=\"20 second\")\n",
    "  .option(\"checkpointLocation\", checkpointFileSyncDir)\n",
    "  .start())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#streamingQuery.stop()\n",
    "streamingQueryFile.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming with eventtime\n",
    "Use generator with following config\n",
    ">`datatype|number|id|{\"min\":0,\"max\":10}`\n",
    "\n",
    ">`datatype|float|temp|{\"min\":20,\"max\":50}`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sparkstream = (SparkSession.builder\n",
    ".appName(\"avgtemp\").getOrCreate())\n",
    "readings = (sparkstream\n",
    "  .readStream.format(\"socket\")\n",
    "  .option(\"host\", \"localhost\")\n",
    "  .option(\"port\", 4000)\n",
    "  .load().withColumn(\"current_timestamp\", current_timestamp()))\n",
    "\n",
    "readings_tab = readings.select(get_json_object(\"value\",\"$.id\").alias(\"id\"),\n",
    "get_json_object(\"value\",\"$.temp\").alias(\"temp\"),\n",
    "timestamp_seconds(round(get_json_object(\"value\",\"$.eventtime\") / 1000)).alias(\"eventtime\"),\n",
    "\"current_timestamp\")\n",
    "\n",
    "readings_tab = readings_tab.withColumn(\"delayed_eventtime\", col(\"eventtime\") - expr(\"INTERVAL 10 seconds\"))\n",
    "\n",
    "eventcol = \"delayed_eventtime\"\n",
    "\n",
    "samples = (readings_tab.withWatermark(eventcol, \"10 seconds\")\n",
    ".groupBy(\"id\", window(eventcol, \"20 second\"))\n",
    ".agg(count(\"id\").alias(\"samples\"),avg(\"temp\").alias(\"averageTemp\"),\n",
    "max(\"current_timestamp\").alias(\"processed_at\"), max(eventcol).alias(\"generated_at\")))\n",
    "\n",
    "## Cooment / uncomment below blocks as per need\n",
    "\n",
    "#streamingQuery = (samples\n",
    "#  .writeStream\n",
    "#  .format(\"console\")\n",
    "#  .option(\"truncate\", \"false\")\n",
    "#  .outputMode(\"append\")\n",
    "#  .trigger(processingTime=\"10 second\")\n",
    "#  .start())\n",
    "#\n",
    "#streamingQueryJsonAppend = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchAppendToJSON10Sec)\n",
    "#  .outputMode(\"append\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonappend10sec\")\n",
    "#  .trigger(processingTime=\"10 second\")\n",
    "#  .start())   \n",
    "\n",
    "streamingQueryJsonAppend = (samples\n",
    "  .writeStream\n",
    "  .foreachBatch(writeMicroBatchAppendToJSON)\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonappend\")\n",
    "  .trigger(processingTime=\"20 second\")\n",
    "  .start())   \n",
    "\n",
    "#streamingQueryJsonUpdate = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchUpdateToJSON)\n",
    "#  .outputMode(\"update\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonupdate\")\n",
    "#  .trigger(processingTime=\"20 second\")\n",
    "#  .start())   \n",
    "\n",
    "#streamingQueryJsonComplete = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchCompleteToJSON)\n",
    "#  .outputMode(\"complete\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsoncomplete\")\n",
    "#  .start())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop Streaming Queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#streamingQueryJsonUpdate.stop()\n",
    "streamingQueryJsonAppend.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfc = spark.read.format(\"delta\").load(\"/tmp/deltaeventscomplete\").cache()\n",
    "dfc.filter(\"id == 2\").show(15, truncate = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/deltaevents\").cache()\n",
    "df.filter(\"id == 2\").show(15, truncate = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "(df.join(dfc,['id','window'], \"fullOuter\")\n",
    "    .select(\"id\", \"window\", dfc.samples.alias(\"complete\"), df.samples,dfc.processed_at)\n",
    "    .orderBy(\"id\")\n",
    "    .filter(\"id == 10\")\n",
    "    .show(50, truncate=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scala excercise from [Sample question](https://databricks-prod-cloudfront.cloud.databricks.com/public/793177bc53e528530b06c78a4fa0e086/0/6221173/100020/latest.html). Translated to Pyspark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Create DF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#schema = StructType([\n",
    "#  StructField(\"name\", StringType(), True),\n",
    "#  StructField(\"department\", IntegerType(), True),\n",
    "#  StructField(\"score\", ArrayType(IntegerType()), True)\n",
    "#])\n",
    "\n",
    "# Works same as above\n",
    "schema = \"name string, department string, score array<int>\"\n",
    "\n",
    "peopleDF = spark.createDataFrame([\n",
    "  (\"Ali\", 0, [100]),\n",
    "  (\"Barbara\", 1, [300, 250, 100]),\n",
    "  (\"Cesar\", 1, [350, 100]),\n",
    "  (\"Dongmei\", 1, [400, 100]),\n",
    "  (\"Eli\", 2, [250]),\n",
    "  (\"Florita\", 2, [500, 300, 100]),\n",
    "  (\"Gatimu\", 3, [300, 100])\n",
    "],schema) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "peopleDF.schema"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(department,StringType,true),StructField(score,ArrayType(IntegerType,true),true)))"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create windowSpec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"escore\").desc())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute using windowSpec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "people = (peopleDF.select(\"*\", explode(\"score\")\n",
    ".alias(\"escore\"))\n",
    ".drop(\"score\")\n",
    ".select(\"name\", expr(\"department *1.0\").alias(\"Dept\"),\"escore\", rank().over(windowSpec).alias(\"maxrank\"))\n",
    ".filter(\"maxrank == 1\")\n",
    ".drop(\"maxrank\")\n",
    ".orderBy(\"department\", ascending=False)\n",
    ".show(5))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----+------+\n",
      "|   name|Dept|escore|\n",
      "+-------+----+------+\n",
      "| Gatimu| 3.0|   300|\n",
      "|Florita| 2.0|   500|\n",
      "|Dongmei| 1.0|   400|\n",
      "|    Ali| 0.0|   100|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "mnmdata = \"/home/kamal/Documents/dev/spark/LearningSparkV2/chapter2/py/src/data/mnm_dataset.csv\"\n",
    "\n",
    "mnmDF = (spark\n",
    ".read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"True\")\n",
    ".option(\"inferschema\", \"True\")\n",
    ".load(mnmdata))\n",
    "\n",
    "resultDF = (mnmDF\n",
    ".groupBy(\"State\",\"Color\")\n",
    ".agg(sum(\"count\").alias(\"Total\"))\n",
    ".orderBy(\"Total\",ascending = False)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "resultDF.count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Show current spark context, get the UI url"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "spark.sparkContext"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.14:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>airline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=airline>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, \n",
    "[\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, \n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    "      ]\n",
    "\n",
    "schema = \"\"\"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, \\\n",
    "`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "blogs = spark.createDataFrame(data,schema)\n",
    "#blogs.write.format(\"json\").save(\"blogs-json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "blogs.withColumn(\"BigHitters\", expr(\"Hits > 10000\")).filter(col(\"First\").endswith(\"i\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+-------+-----------------+---------+-----+--------------------+----------+\n",
      "| Id|First|   Last|              Url|Published| Hits|           Campaigns|BigHitters|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+----------+\n",
      "|  5|Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|      true|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fire Department Dataset excercise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "firecallsDF = (spark\n",
    ".read\n",
    ".format(\"csv\")\n",
    ".option(\"samplingRatio\", .001)\n",
    ".option(\"header\", True)\n",
    ".load(\"databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"))\n",
    "\n",
    "firecallsDF.select(\"CallType\").distinct().show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n",
      "|            CallType|\n",
      "+--------------------+\n",
      "|Elevator / Escala...|\n",
      "|         Marine Fire|\n",
      "|  Aircraft Emergency|\n",
      "|Confined Space / ...|\n",
      "|      Administrative|\n",
      "|              Alarms|\n",
      "|Odor (Strange / U...|\n",
      "|Citizen Assist / ...|\n",
      "|              HazMat|\n",
      "|Watercraft in Dis...|\n",
      "|           Explosion|\n",
      "|           Oil Spill|\n",
      "|        Vehicle Fire|\n",
      "|  Suspicious Package|\n",
      "|Extrication / Ent...|\n",
      "|               Other|\n",
      "|        Outside Fire|\n",
      "|   Traffic Collision|\n",
      "|       Assist Police|\n",
      "|Gas Leak (Natural...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from pyspark.sql.functions import *\n",
    "new_fire_df = (firecallsDF\n",
    ".select(\"*\",\n",
    "    to_timestamp(col(\"CallDate\"),\"MM/dd/yyyy\").alias(\"IncidentDate\"),\n",
    "    to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\").alias(\"OnWatchDate\"),\n",
    "    to_timestamp(col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\").alias(\"AvailableDtTs\"))\n",
    ".where(col(\"Incidentdate\").isNotNull())\n",
    ".drop(\"CallDate\")\n",
    ".drop(\"WatchDate\")\n",
    ".drop(\"AvailableDtTm\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which calltype generated most calls"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "(firecallsDF\n",
    ".select(\"CallType\")\n",
    ".filter(col(\"CallType\").isNotNull())\n",
    ".groupBy(\"CallType\")\n",
    ".agg(count(\"CallType\").alias(\"count\"))\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show(30, truncate = False))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------+------+\n",
      "|CallType                                    |count |\n",
      "+--------------------------------------------+------+\n",
      "|Medical Incident                            |113794|\n",
      "|Structure Fire                              |23319 |\n",
      "|Alarms                                      |19406 |\n",
      "|Traffic Collision                           |7013  |\n",
      "|Citizen Assist / Service Call               |2524  |\n",
      "|Other                                       |2166  |\n",
      "|Outside Fire                                |2094  |\n",
      "|Vehicle Fire                                |854   |\n",
      "|Gas Leak (Natural and LP Gases)             |764   |\n",
      "|Water Rescue                                |755   |\n",
      "|Odor (Strange / Unknown)                    |490   |\n",
      "|Electrical Hazard                           |482   |\n",
      "|Elevator / Escalator Rescue                 |453   |\n",
      "|Smoke Investigation (Outside)               |391   |\n",
      "|Fuel Spill                                  |193   |\n",
      "|HazMat                                      |124   |\n",
      "|Industrial Accidents                        |94    |\n",
      "|Explosion                                   |89    |\n",
      "|Train / Rail Incident                       |57    |\n",
      "|Aircraft Emergency                          |36    |\n",
      "|Assist Police                               |35    |\n",
      "|High Angle Rescue                           |32    |\n",
      "|Watercraft in Distress                      |28    |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|28    |\n",
      "|Oil Spill                                   |21    |\n",
      "|Suspicious Package                          |15    |\n",
      "|Marine Fire                                 |14    |\n",
      "|Confined Space / Structure Collapse         |13    |\n",
      "|Mutual Aid / Assist Outside Agency          |9     |\n",
      "|Administrative                              |3     |\n",
      "+--------------------------------------------+------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What were all the different types of fire calls in 2018?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "(new_fire_df\n",
    ".select(\"CallType\",year(\"IncidentDate\").alias(\"Year\"))\n",
    ".filter(\"Year == 2018\")\n",
    ".groupBy(\"CallType\")\n",
    ".agg(count(\"CallType\").alias(\"count\"))\n",
    ".show(2)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n",
      "|            CallType|count|\n",
      "+--------------------+-----+\n",
      "|Elevator / Escala...|   36|\n",
      "|              Alarms| 1144|\n",
      "+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "new_fire_df.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CallNumber',\n",
       " 'UnitID',\n",
       " 'IncidentNumber',\n",
       " 'CallType',\n",
       " 'CallFinalDisposition',\n",
       " 'Address',\n",
       " 'City',\n",
       " 'Zipcode',\n",
       " 'Battalion',\n",
       " 'StationArea',\n",
       " 'Box',\n",
       " 'OriginalPriority',\n",
       " 'Priority',\n",
       " 'FinalPriority',\n",
       " 'ALSUnit',\n",
       " 'CallTypeGroup',\n",
       " 'NumAlarms',\n",
       " 'UnitType',\n",
       " 'UnitSequenceInCallDispatch',\n",
       " 'FirePreventionDistrict',\n",
       " 'SupervisorDistrict',\n",
       " 'Neighborhood',\n",
       " 'Location',\n",
       " 'RowID',\n",
       " 'Delay',\n",
       " 'IncidentDate',\n",
       " 'OnWatchDate',\n",
       " 'AvailableDtTs']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which neighborhood in San Francisco generated the most fire calls in 2018"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "(new_fire_df\n",
    ".select(\"city\",\"Neighborhood\",\"CallType\",year(\"IncidentDate\").alias(\"Year\"))\n",
    ".where(\"city in ('San Francisco','SF', 'SAN FRANCISCO')\")\n",
    ".where(col(\"Year\") == 2018)  #Can also be written as \"Year == 2018\"\n",
    ".where(col(\"CallType\").like('%Fire%'))\n",
    ".groupBy(\"Neighborhood\")\n",
    ".agg(count(\"Neighborhood\").alias(\"TotalCalls\"))\n",
    ".orderBy(\"TotalCalls\",ascending=False)\n",
    ".show(3, truncate=False))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+----------+\n",
      "|Neighborhood                  |TotalCalls|\n",
      "+------------------------------+----------+\n",
      "|Tenderloin                    |105       |\n",
      "|Financial District/South Beach|92        |\n",
      "|Mission                       |80        |\n",
      "+------------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def cube(s):\n",
    "    return s*s*s\n",
    "\n",
    "spark.udf.register(\"cube\", cube, LongType())\n",
    "\n",
    "spark.range(1,5).createOrReplaceTempView(\"udf_test\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 18:17:09 WARN SimpleFunctionRegistry: The function cube replaced a previously registered function.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "spark.sql(\"select id, cube(id) from udf_test\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------+\n",
      "| id|cube(id)|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "cubed_udf = pandas_udf(cube, LongType())\n",
    "df_id = spark.range(1,4)\n",
    "df_id.select(\"id\",cubed_udf(\"id\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------+\n",
      "| id|cube(id)|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "people_listscore = peopleDF.select(\"name\",\"department\",explode(\"score\").alias(\"score\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "people_listscore.groupBy(\"name\").agg(collect_list(\"score\")).show(20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-------------------+\n",
      "|   name|collect_list(score)|\n",
      "+-------+-------------------+\n",
      "| Gatimu|         [300, 100]|\n",
      "|Barbara|    [300, 250, 100]|\n",
      "|  Cesar|         [350, 100]|\n",
      "|Florita|    [500, 300, 100]|\n",
      "|    Eli|              [250]|\n",
      "|Dongmei|         [400, 100]|\n",
      "|    Ali|              [100]|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath = \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airportsnaFilePath =  \"databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    "  \n",
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "  .load(airportsnaFilePath))\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "departureDelays = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .options(header=\"true\")\n",
    "  .load(tripdelaysFilePath))\n",
    "\n",
    "departureDelays = (departureDelays\n",
    ".withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    ".withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"select filter(score, s -> s = 250) as new_score from global_temp.peopleDF\").show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "spark.sql(\"set -v\").select(\"key\",\"value\").show(truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------------------------+--------------------------------------------------------------------+\n",
      "|key                                                        |value                                                               |\n",
      "+-----------------------------------------------------------+--------------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes            |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>    |\n",
      "|spark.sql.adaptive.coalescePartitions.enabled              |true                                                                |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum  |<undefined>                                                         |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum      |<undefined>                                                         |\n",
      "|spark.sql.adaptive.enabled                                 |false                                                               |\n",
      "|spark.sql.adaptive.localShuffleReader.enabled              |true                                                                |\n",
      "|spark.sql.adaptive.optimizer.excludedRules                 |<undefined>                                                         |\n",
      "|spark.sql.adaptive.skewJoin.enabled                        |true                                                                |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionFactor          |5                                                                   |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes|256MB                                                               |\n",
      "|spark.sql.ansi.enabled                                     |false                                                               |\n",
      "|spark.sql.autoBroadcastJoinThreshold                       |10MB                                                                |\n",
      "|spark.sql.avro.compression.codec                           |snappy                                                              |\n",
      "|spark.sql.avro.deflate.level                               |-1                                                                  |\n",
      "|spark.sql.avro.filterPushdown.enabled                      |true                                                                |\n",
      "|spark.sql.broadcastTimeout                                 |300                                                                 |\n",
      "|spark.sql.bucketing.coalesceBucketsInJoin.enabled          |false                                                               |\n",
      "|spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio   |4                                                                   |\n",
      "|spark.sql.cache.serializer                                 |org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer|\n",
      "|spark.sql.catalog.spark_catalog                            |<undefined>                                                         |\n",
      "+-----------------------------------------------------------+--------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "df_big_range2 = spark.range(1000000).repartition(8)\n",
    "df_big_range2.rdd.getNumPartitions()\n",
    "df_big_range2.cache()\n",
    "df_big_range2.count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 20:55:24 WARN CacheManager: Asked to cache already cached data.\n",
      "21/09/26 20:55:28 WARN MemoryStore: Not enough space to cache rdd_176_4 in memory! (computed 48.2 MiB so far)\n",
      "21/09/26 20:55:28 WARN BlockManager: Persisting block rdd_176_4 to disk instead.\n",
      "21/09/26 20:55:55 WARN MemoryStore: Not enough space to cache rdd_176_4 in memory! (computed 48.2 MiB so far)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 42
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 20:56:00 WARN MemoryStore: Not enough space to cache rdd_176_6 in memory! (computed 48.2 MiB so far)\n",
      "21/09/26 20:56:01 WARN MemoryStore: Not enough space to cache rdd_176_5 in memory! (computed 48.2 MiB so far)\n",
      "21/09/26 20:56:04 WARN MemoryStore: Not enough space to cache rdd_176_7 in memory! (computed 48.2 MiB so far)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "df_big_range2.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}