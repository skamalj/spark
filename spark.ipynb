{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Install Java (I am on 11), Spark and Python (I have 3.8). \n",
    "\n",
    "Install Jupyter server and jupyter plugin in vscode.\n",
    "\n",
    "Set env variables as below (Change values according to your setup)\n",
    "\n",
    "```\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "export SPARK_HOME=/home/kamal/spark-3.1.2-bin-hadoop3.2\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "export PYTHONPATH=${SPARK_HOME}/python/:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):${PYTHONPATH}\n",
    "```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Sessions\n",
    "## Create Spark Session"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"airline\").getOrCreate())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Delta spark session\n",
    "* This requires delta lake jar be downloaded from mavenrepository and placed in SPARK_HOME/jars. Restart Jupyter server after that.  \n",
    "* Install delta-spark package (pip install delta-spark)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "delta_spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and operate on Data\n",
    "## Read CSV with string schema\n",
    "Data used here is available from databricks github [page](https://github.com/databricks/LearningSparkV2.git)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "csv_file = \"./databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "schema = \"date string, delay int, distance int, origin string, destination string\"\n",
    "df = (spark.read\n",
    ".schema(schema)\n",
    ".option(\"header\", \"true\")\n",
    ".csv(csv_file)).cache()\n",
    "df.printSchema()\n",
    "df.first()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Operate on Columns, with col"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.select(col(\"distance\") / 1000).show(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using withColumn()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.withColumn(\"calculatedCol\", col(\"distance\")/1000).show(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timestamp from millisecond (since epoch)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"select timestamp_millis(1631163333673) as timestampdata\").show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with JSON\n",
    "\n",
    "### Extract with from_json()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = [(1, '''{\"a\": 1,\"b\": 2.3}''')]\n",
    "schema = 'a INT, b FLOAT'\n",
    "df_json = spark.createDataFrame(data, (\"key\", \"value\"))\n",
    "df_json.select(from_json(df_json.value, schema)[\"a\"].alias(\"id\"), \n",
    "from_json(df_json.value, schema)[\"b\"].alias(\"value\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract with get_json_object()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_json.select(get_json_object(\"value\",\"$.a\").alias(\"id\"), \n",
    "get_json_object(\"value\",\"$.b\").alias(\"value\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.printSchema()\n",
    "from pyspark.sql.functions import col\n",
    "df.select(\"distance\",\"origin\",\"destination\").filter(col(\"distance\") > 1000).distinct().show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Views\n",
    "* Global views are used, in queries, with \"global_temp\" prefix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.createOrReplaceTempView(\"Airline\")\n",
    "df.createOrReplaceGlobalTempView(\"GlobalAirlineView\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"select * from Airline limit 10\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination, \n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay >= 120 AND delay <= 360 THEN 'Long Delays'\n",
    "                  WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM Airline\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"\"\"SELECT *,\n",
    "                CASE\n",
    "                    WHEN delay < 0 THEN 'EARLY'\n",
    "                    WHEN delay == 0 THEN 'ON-TIME'\n",
    "                    ELSE 'DELAY'\n",
    "                END AS FL_Delay\n",
    "             FROM Airline\"\"\").show(6)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add column to DF\n",
    "## Use multiple conditions on column with when-otherwise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn('FL_delay', when(col(\"delay\") > 0, 'DELAYED')\n",
    ".when(col(\"delay\") < 0, 'EARLY')\n",
    ".otherwise('ON-TIME')).show(6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set shuffle partitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",4)\n",
    "df.write.option(\"path\",\"/home/kamal/airline_data\").format(\"csv\").saveAsTable(\"Airline\")\n",
    "df.rdd.getNumPartitions()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create, Use and drop databases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.sql(\"drop database if exists newdb cascade\")\n",
    "spark.sql(\"create database newdb location '/home/kamal/spark-warehouse/newdatabase'\")\n",
    "spark.catalog.listDatabases()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(\"use newdb\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working with AVRO\n",
    "Avro package is still external (but supported), so needs to be downloaded and placed in classpath. Download spark-avro jar from [mavenrepo](https://mvnrepository.com/artifact/org.apache.spark/spark-avro_2.12/3.1.2),  and place it in $SPARK_HOME/jars. Then restart Jupyter server "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.conf.set(\"spark.jars.packages\", \"com.databricks:spark-avro_2.12:3.1.2\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n",
    "    USING com.databricks.spark.avro\n",
    "    OPTIONS (\n",
    "      path \"./databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "    )\n",
    "\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_fl_summary = spark.sql(\"select * from episode_tbl\")\n",
    "df_fl_summary.count()\n",
    "df_fl_summary.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UDFs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import LongType\n",
    "import pandas as pd\n",
    "def cubed(s: pd.Series) -> pd.Series:\n",
    "    return s*s*s\n",
    "\n",
    "df_range = spark.range(1,9)    \n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_range.select(\"id\",cubed_udf(\"id\").alias(\"cubed\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.udf.register(\"cubed_udf\",cubed_udf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_range.createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql(\"select id, cubed_udf(id) from udf_test\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delta Read and Write"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = delta_spark.sql(\"select * from global_temp.GlobalAirlineView\").cache()\n",
    "data.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(\"/tmp/delta-table/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spark Streaming\n",
    "\n",
    "## foreachBatch Function\n",
    "\n",
    "foreachBatch lets us use \"update\", \"append\" and \"complete\" output modes. Whereas file sink only allows append."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import partial\n",
    "def foreachBatchFunction(format, location, df, batchId):\n",
    "        (df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(format)\n",
    "        .save(location+str(batchId)))\n",
    "writeMicroBatchUpdateToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonUpdate/\")\n",
    "writeMicroBatchAppendToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonAppend/\")\n",
    "writeMicroBatchCompleteToJSON = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonComplete/\")\n",
    "writeMicroBatchAppendToJSON10Sec = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/jsonAppend10sec/\")\n",
    "writeMicroBatchToCSV = partial(foreachBatchFunction,\"JSON\",\"/home/kamal/sparkcsv/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Count\n",
    "Generate test dat using [datastreamer](https://github.com/skamalj/datagenerator) - with followung config. It creates space separated list of  6 words in each line\n",
    ">`lorem|words|textline|6`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sparkstream = (SparkSession.builder\n",
    ".appName(\"airline\").getOrCreate())\n",
    "lines = (sparkstream\n",
    "  .readStream.format(\"socket\")\n",
    "  .option(\"host\", \"localhost\")\n",
    "  .option(\"port\", 4000)\n",
    "  .load())\n",
    "\n",
    "linesnojson = lines.select(get_json_object(\"value\",\"$.textline\").alias(\"line\"))\n",
    "\n",
    "words = linesnojson.select(explode(split(col(\"line\"), \" \")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "\n",
    "checkpointDir = \"/home/kamal/sparkcheckpoint\"\n",
    "checkpointFileSyncDir = \"/home/kamal/sparkcheckpointcsv\"\n",
    "\n",
    "#streamingQuery = (counts\n",
    "#  .writeStream\n",
    "#  .format(\"console\")\n",
    "#  .outputMode(\"complete\")\n",
    "#  .trigger(processingTime=\"20 second\")\n",
    "#  .option(\"checkpointLocation\", checkpointDir)\n",
    "#  .start())\n",
    "\n",
    "streamingQueryFile = (counts\n",
    "  .writeStream\n",
    "  .foreachBatch(writeMicroBatchToCSV)\n",
    "  .outputMode(\"update\")\n",
    "  .trigger(processingTime=\"20 second\")\n",
    "  .option(\"checkpointLocation\", checkpointFileSyncDir)\n",
    "  .start())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#streamingQuery.stop()\n",
    "streamingQueryFile.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming with eventtime\n",
    "Use generator with following config\n",
    ">`datatype|number|id|{\"min\":0,\"max\":10}`\n",
    "\n",
    ">`datatype|float|temp|{\"min\":20,\"max\":50}`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "sparkstream = (SparkSession.builder\n",
    ".appName(\"avgtemp\").getOrCreate())\n",
    "readings = (sparkstream\n",
    "  .readStream.format(\"socket\")\n",
    "  .option(\"host\", \"localhost\")\n",
    "  .option(\"port\", 4000)\n",
    "  .load().withColumn(\"current_timestamp\", current_timestamp()))\n",
    "\n",
    "readings_tab = readings.select(get_json_object(\"value\",\"$.id\").alias(\"id\"),\n",
    "get_json_object(\"value\",\"$.temp\").alias(\"temp\"),\n",
    "timestamp_seconds(round(get_json_object(\"value\",\"$.eventtime\") / 1000)).alias(\"eventtime\"),\n",
    "\"current_timestamp\")\n",
    "\n",
    "readings_tab = readings_tab.withColumn(\"delayed_eventtime\", col(\"eventtime\") - expr(\"INTERVAL 10 seconds\"))\n",
    "\n",
    "eventcol = \"delayed_eventtime\"\n",
    "\n",
    "samples = (readings_tab.withWatermark(eventcol, \"10 seconds\")\n",
    ".groupBy(\"id\", window(eventcol, \"20 second\"))\n",
    ".agg(count(\"id\").alias(\"samples\"),avg(\"temp\").alias(\"averageTemp\"),\n",
    "max(\"current_timestamp\").alias(\"processed_at\"), max(eventcol).alias(\"generated_at\")))\n",
    "\n",
    "## Cooment / uncomment below blocks as per need\n",
    "\n",
    "#streamingQuery = (samples\n",
    "#  .writeStream\n",
    "#  .format(\"console\")\n",
    "#  .option(\"truncate\", \"false\")\n",
    "#  .outputMode(\"append\")\n",
    "#  .trigger(processingTime=\"10 second\")\n",
    "#  .start())\n",
    "#\n",
    "#streamingQueryJsonAppend = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchAppendToJSON10Sec)\n",
    "#  .outputMode(\"append\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonappend10sec\")\n",
    "#  .trigger(processingTime=\"10 second\")\n",
    "#  .start())   \n",
    "\n",
    "streamingQueryJsonAppend = (samples\n",
    "  .writeStream\n",
    "  .foreachBatch(writeMicroBatchAppendToJSON)\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonappend\")\n",
    "  .trigger(processingTime=\"20 second\")\n",
    "  .start())   \n",
    "\n",
    "#streamingQueryJsonUpdate = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchUpdateToJSON)\n",
    "#  .outputMode(\"update\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsonupdate\")\n",
    "#  .trigger(processingTime=\"20 second\")\n",
    "#  .start())   \n",
    "\n",
    "#streamingQueryJsonComplete = (samples\n",
    "#  .writeStream\n",
    "#  .foreachBatch(writeMicroBatchCompleteToJSON)\n",
    "#  .outputMode(\"complete\")\n",
    "#  .option(\"checkpointLocation\", \"/home/kamal/spark/checkpoint/jsoncomplete\")\n",
    "#  .start())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop Streaming Queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#streamingQueryJsonUpdate.stop()\n",
    "streamingQueryJsonAppend.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfc = spark.read.format(\"delta\").load(\"/tmp/deltaeventscomplete\").cache()\n",
    "dfc.filter(\"id == 2\").show(15, truncate = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/deltaevents\").cache()\n",
    "df.filter(\"id == 2\").show(15, truncate = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "(df.join(dfc,['id','window'], \"fullOuter\")\n",
    "    .select(\"id\", \"window\", dfc.samples.alias(\"complete\"), df.samples,dfc.processed_at)\n",
    "    .orderBy(\"id\")\n",
    "    .filter(\"id == 10\")\n",
    "    .show(50, truncate=False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scala excercise from [Sample question](https://databricks-prod-cloudfront.cloud.databricks.com/public/793177bc53e528530b06c78a4fa0e086/0/6221173/100020/latest.html). Translated to Pyspark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Create DF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"department\", IntegerType(), True),\n",
    "  StructField(\"score\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "peopleDF = spark.createDataFrame([\n",
    "  (\"Ali\", 0, [100]),\n",
    "  (\"Barbara\", 1, [300, 250, 100]),\n",
    "  (\"Cesar\", 1, [350, 100]),\n",
    "  (\"Dongmei\", 1, [400, 100]),\n",
    "  (\"Eli\", 2, [250]),\n",
    "  (\"Florita\", 2, [500, 300, 100]),\n",
    "  (\"Gatimu\", 3, [300, 100])\n",
    "],schema) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "peopleDF.schema"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create windowSpec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"escore\").desc())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute using windowSpec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(peopleDF.select(\"*\", explode(\"score\")\n",
    ".alias(\"escore\"))\n",
    ".drop(\"score\")\n",
    ".select(\"name\", \"department\",\"escore\", rank().over(windowSpec).alias(\"maxrank\"))\n",
    ".filter(\"maxrank == 1\")\n",
    ".drop(\"maxrank\")\n",
    ".orderBy(\"department\")\n",
    ".show())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}